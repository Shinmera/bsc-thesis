\section{NEXMark Benchmark\cite{nexmark}}
NEXMark is an evolution of the XMark benchmark. XMark was initially designed for relational databases and defines a small schema for an online auction house. NEXMark builds on this idea and presents a schema of three concrete tables, and a set of queries to run in a streaming sense. NEXMark attempts to provide a benchmark that is both extensive in its use of operators, and close to a real-world application by being grounded in a well-known problem. The original benchmark proposed by Tucker et al. was adopted and extended by the Apache Foundation for their use in Beam\cite{nexmark-beam}, a system intended to provide a general API for a variety of streaming systems. We will follow the Beam implementation, as it is the most widely adopted one, despite having several differences to the benchmark originally outlined in the paper. See \autoref{section:nexmark-remarks} for an outline of the differences we found. \\

The benchmark defines the following queries:

\begin{enumerate}
  \setcounter{enumi}{-1}
\item \tb{Pass-Through} This is similar to HiBench's Identity query and should just output the received data.
  \imagefigure[nexmark-0]{images/nexmark-0.pdf}{NEXMark's Query 0.}
\item \tb{Currency Conversion} Output bids on auctions, but translate the bid price to Euro.
  \imagefigure[nexmark-1]{images/nexmark-1.pdf}{NEXMark's Query 1.}
\item \tb{Selection} Filter to auctions with a specific set of IDs.
  \imagefigure[nexmark-2]{images/nexmark-2.pdf}{NEXMark's Query 2.}
\item \tb{Local Item Suggestion} Output persons that are outputting auctions in particular states.
  \imagefigure[nexmark-3]{images/nexmark-3.pdf}{NEXMark's Query 3.}
\item \tb{Average Price for a Category} Compute the average auction price in a category for all auctions that haven't expired yet.
  \imagefigure[nexmark-4]{images/nexmark-4.pdf}{NEXMark's Query 4.}
\item \tb{Hot Items} Show the auctions with the most bids over the last hour, updated every minute.
  \imagefigure[nexmark-5]{images/nexmark-5.pdf}{NEXMark's Query 5.}
\item \tb{Average Selling Price by Seller} Compute the average selling price for the last ten closed auctions per auctioner.
  \imagefigure[nexmark-6]{images/nexmark-6.pdf}{NEXMark's Query 6.}
\item \tb{Highest Bid} Output the auction and bid with the highest price in the last minute.
  \imagefigure[nexmark-7]{images/nexmark-7.pdf}{NEXMark's Query 7.}
\item \tb{Monitor New Users} Show persons that have opened an auction in the last 12 hours.
  \imagefigure[nexmark-8]{images/nexmark-8.pdf}{NEXMark's Query 8.}
\item \tb{Winning Bids} Compute the winning bid for an auction. This is used in queries 4 and 6.
  \imagefigure[nexmark-9]{images/nexmark-9.pdf}{NEXMark's Query 9.}
\item \tb{Log to GCS} Output all events to a GCS file, which is supposed to illustrate large side effects.
  \imagefigure[nexmark-10]{images/nexmark-10.pdf}{NEXMark's Query 10.}
\item \tb{Bids in a Session} Show the number of bids a person has made in their session.
  \imagefigure[nexmark-11]{images/nexmark-11.pdf}{NEXMark's Query 11.}
\item \tb{Bids within a Window} Compute the number of bids a user makes within a processing-time constrained window.
  \imagefigure[nexmark-12]{images/nexmark-12.pdf}{NEXMark's Query 12.}
\end{enumerate}

The queries are based on three types of events that can enter the system: \code{Person}, \code{Auction}, and \code{Bid}. Their fields are as follows:

\paragraph*{Person}
\begin{itemize}
\item \code{id} A person-unique integer ID.
\item \code{name} A string for the person's full name.
\item \code{email_address} The person's email address as a string.
\item \code{credit_card} The credit card number as a 19-letter string.
\item \code{city} One of several US city names as a string.
\item \code{state} One of several US states as a two-letter string.
\item \code{date_time} A millisecond timestamp for the event origin.
\end{itemize}

\paragraph*{Auction}
\begin{itemize}
\item \code{id} An auction-unique integer ID.
\item \code{item_name} The name of the item being auctioned.
\item \code{description} A short description of the item.
\item \code{initial_bid} The initial bid price in cents.
\item \code{reserve} ???
\item \code{date_time} A millisecond timestamp for the event origin.
\item \code{expires} A UNIX epoch timestamp for the expiration date of the auction.
\item \code{seller} The ID of the person that created this auction.
\item \code{category} The ID of the category this auction belongs to.
\end{itemize}

\paragraph*{Bid}
\begin{itemize}
\item \code{auction} The ID of the auction this bid is for.
\item \code{bidder} The ID of the person that placed this bid.
\item \code{price} The price in cents that the person bid for.
\item \code{date_time} A millisecond timestamp for the event origin.
\end{itemize}

\subsection{Implementation}
\subsubsection{Data Generation}
In order to be able to run the benchmark outside of the Beam framework, we had to replicate their generator. For this we translated the original Java sources of the generator (\code{sdks/java/nexmark/src/main/java/org/apache/beam/sdk/nexmark/sources/generator}) into Rust. Unfortunately it appears that the Beam generator has a hard constraint on its rate due to a lack of precision, and can thus only output at most 2M events per second. In our tests the system did not congest at these rates, so we had to modify the generation to be more precise, and allow higher rates. This may have changed the generation in subtle ways that we are not aware of. If it is indeed possible to output more than 2M events per second in the original generator, we could not figure out how to make it do so due to the lack of proper documentation.

\subsubsection{Queries}
\paragraph{Query 0}
\begin{listing}[H]
  \inputminted[firstline=396,lastline=396]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 0}
  \label{lst:nexmark-query0}
\end{listing}

\paragraph{Query 1}
\begin{listing}[H]
  \inputminted[firstline=422,lastline=424]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 1}
  \label{lst:nexmark-query1}
\end{listing}

\paragraph{Query 2}
\begin{listing}[H]
  \inputminted[firstline=457,lastline=460]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 2}
  \label{lst:nexmark-query2}
\end{listing}

\paragraph{Query 3}
\begin{listing}[H]
  \inputminted[firstline=492,lastline=501]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 3}
  \label{lst:nexmark-query3}
\end{listing}

\paragraph{Query 4}
\begin{listing}[H]
  \inputminted[firstline=533,lastline=535]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 4}
  \label{lst:nexmark-query4}
\end{listing}

\paragraph{Query 5}
\begin{listing}[H]
  \inputminted[firstline=561,lastline=570]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 5}
  \label{lst:nexmark-query5}
\end{listing}

\paragraph{Query 6}
\begin{listing}[H]
  \inputminted[firstline=602,lastline=603]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 6}
  \label{lst:nexmark-query6}
\end{listing}

\paragraph{Query 7}
\begin{listing}[H]
  \inputminted[firstline=635,lastline=641]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 7}
  \label{lst:nexmark-query7}
\end{listing}

\paragraph{Query 8}
\begin{listing}[H]
  \inputminted[firstline=673,lastline=682]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 8}
  \label{lst:nexmark-query8}
\end{listing}

\paragraph{Query 9}
\begin{listing}[H]
  \inputminted[firstline=701,lastline=737]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 9}
  \label{lst:nexmark-query9}
\end{listing}

\paragraph{Query 10}
We did not implement Query 10 as we felt it did not reflect a useful case outside of the very specific and particular application of writing to a Google Cloud Storage file.

\paragraph{Query 11}
\begin{listing}[H]
  \inputminted[firstline=784,lastline=787]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 11}
  \label{lst:nexmark-query11}
\end{listing}

\paragraph{Query 12}
\begin{listing}[H]
  \inputminted[firstline=811,lastline=818]{rust}{benchmarks/src/nexmark.rs}
  \caption{Implementation for NEXMark's Query 12}
  \label{lst:nexmark-query12}
\end{listing}

\subsection{Evaluation}
\latencyfigure[nex]{
   \addplot table[y index=0, x index=6] {data/latency.csv};
   \addplot table[y index=0, x index=7] {data/latency.csv};
   \addplot table[y index=0, x index=8] {data/latency.csv};
   \addplot table[y index=0, x index=9] {data/latency.csv};
   \addplot table[y index=0, x index=10] {data/latency.csv};
   \addplot table[y index=0, x index=11] {data/latency.csv};
   \addplot table[y index=0, x index=12] {data/latency.csv};
   \addplot table[y index=0, x index=13] {data/latency.csv};
   \addplot table[y index=0, x index=14] {data/latency.csv};
   \addplot table[y index=0, x index=15] {data/latency.csv};
   \addplot table[y index=0, x index=16] {data/latency.csv};
   %\addplot table[y index=0, x index=17] {data/latency.csv};
   \legend{Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q11}
}{Linlog plot of the Latency vs Rate.}

\scalingfigure[nex]{
  \addplot table[x index=0, y index=6] {data/scaling.csv};
  \addplot table[x index=0, y index=7] {data/scaling.csv};
  \addplot table[x index=0, y index=8] {data/scaling.csv};
  \addplot table[x index=0, y index=9] {data/scaling.csv};
  \addplot table[x index=0, y index=10] {data/scaling.csv};
  \addplot table[x index=0, y index=11] {data/scaling.csv};
  \addplot table[x index=0, y index=12] {data/scaling.csv};
  \addplot table[x index=0, y index=13] {data/scaling.csv};
  \addplot table[x index=0, y index=14] {data/scaling.csv};
  \addplot table[x index=0, y index=15] {data/scaling.csv};
  \addplot table[x index=0, y index=16] {data/scaling.csv};
  %\addplot table[x index=0, y index=17] {data/scaling.csv};
  \legend{Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q11}
}{Linlog plot of the scaling behaviour.}

\cdffigure[nex]{
  \addplot table[y index=0, x index=6] {data/cdf.csv};
  \addplot table[y index=0, x index=7] {data/cdf.csv};
  \addplot table[y index=0, x index=8] {data/cdf.csv};
  \addplot table[y index=0, x index=9] {data/cdf.csv};
  \addplot table[y index=0, x index=10] {data/cdf.csv};
  \addplot table[y index=0, x index=11] {data/cdf.csv};
  \addplot table[y index=0, x index=12] {data/cdf.csv};
  \addplot table[y index=0, x index=13] {data/cdf.csv};
  \addplot table[y index=0, x index=14] {data/cdf.csv};
  \addplot table[y index=0, x index=15] {data/cdf.csv};
  \addplot table[y index=0, x index=16] {data/cdf.csv};
  %\addplot table[y index=0, x index=17] {data/cdf.csv};
  \legend{Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q11}
}{CDF plot.}

\subsection{Remarks}\label{section:nexmark-remarks}
NEXMark presents the most extensive benchmark of the three we have investigated. It shows a number of different applications that involve a variety of different operators and configurations. Unlike the other two, NEXMark was a research project designed to present a useful benchmark for streaming systems. As such it has a formal specification of the datastructures and queries involved, and presents a reference data generation implementation. These are vital pieces if it should be possible for third-parties to implement the benchmark and compare results. \\

However, due to the lack of impact of the original paper, the only widespread use of the benchmark is with Apache's Beam system. The Beam implementors made several relatively severe changes to the benchmark. First, they implemented their own generator that has almost nothing in common with the original generator. Second, they added more queries whose precise behaviour and purpose is not formally specified or documented anywhere. Third, they changed the size of the windows to be merely ten seconds, rather than the minutes and hours the original specification sets. \\

We assume the idea behind Beam's implementation is that, in order to offer comparable benchmarks for systems, you would simply have to write a backend for your system in Beam. This however is not trivially achievable, and also will not actually produce results that will properly reflect your system, as the benchmark will implicitly measure and compare not just your system on its own, but also the backend you wrote for Beam and how well it translates queries. \\

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
