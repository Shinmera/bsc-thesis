\section{Conclusion}
As part of this thesis we have surveyed and evaluated a number of papers describing other, current streaming systems. We specifically looked at the ways in which these systems are used and tested. We found that most tests involve relatively simple data flows and setups, with the exception perhaps being ``\citetitle{storm2}''\cite{storm2}. \\

We did not find any consensus on what constitutes a representative, exhaustive, or even just generally sufficient test for a streaming system. The word count example seems to be the most prevalent, but we do not believe that it provides meaningful data for the evaluation of a streaming system when compared to practical problems a system should be capable of solving. \\

Furthermore we found a disturbing lack of reproducibility of test setups. A large number of papers use data for their experiments that is neither public, nor described in sufficient detail. We strongly believe that authors should either use or provide publicly available datasets, or use well-defined generated data with a publicly available generation algorithm. Without any data available, it is not feasible for peers to validate experiments and make meaningful conclusions about the differences between systems. \\

For our work we implemented three benchmarks for Timely that have been implemented on a variety of other streaming systems. These benchmarks seem to be among the most widely used to evaluate and contrast performance between systems. To do so we had to implement a number of additional operators in order to concisely express the data flows used in the benchmarks. Thanks to Timely's construction and Rust's expressiveness it is however no big challenge to add even complex operators to the system that nevertheless perform well. \\

In our closed-loop experiments evaluating the benchmarks on a 32-core system we found Timely capable of scaling up to tens of millions of events per second before reaching its maximum throughput, after which it is not able to keep up with the input rate of events. We believe that this shows great promise for the performance, scalability, and usability of the Timely system as a streaming data processor. \\

Unfortunately we have also come to the conclusion that most of these benchmarks aren't very useful for the overall evaluation of streaming systems. They suffer from a lack of specification and documentation (\hyperref[section:hibench-remarks]{HiBench}, \hyperref[section:nexmark-remarks]{NEXMark}), include a variety of very questionable operations in their data flows (\hyperref[section:hibench-remarks]{HiBench}, \hyperref[section:ysb-remarks]{YSB}), or do not include data flows that are complex enough to evaluate the system in a meaningful way (\hyperref[section:hibench-remarks]{HiBench}, \hyperref[section:ysb-remarks]{YSB}). Please see the respective remarks sections for a more detailed discussion of the problems we have found. \\

Based on our experience implementing the aforementioned benchmarks, we believe that the following traits are vital for the definition of a future, useful, and meaningful benchmark:

\begin{itemize}
\item Both the input \textit{and output} data schema of each data flow are specified clearly using an abstract specification language. Without this verifying correctness of an implementation is not trivial.
\item Workloads are generated according to well-specified, deterministic algorithms with clearly defined parameters and effects. Especially the use of random number generators should be either avoided entirely, or a very specific random number generation algorithm should be specified in detail to be used with the data generation. This is necessary in order to ensure reproducible setups and to make it feasible to verify implementation correctness.
\item No specific external system requirements for feeding data into the system, consuming data from the system, or performing any part of the data flow computation. Requiring specific external systems complicates experiment setup, risks implementation bitrot, and introduces outside variables into the evaluation that might significantly bias the performance evaluation, or even bottleneck it.
\item Data flows are specified in an abstract modelling language that is well defined. Without a precise definition of the data flows it is impossible to verify whether an implementation is doing the right thing or not. Providing reference implementations for data flows is not acceptable, as the implementation might contain subtle bugs or exhibit other properties of the system that make validation confusing at best, and impossible at worst.
\item Each data flow is accompanied by a verified reference output data set using the default parameters. The data set should be available in a common, machine-readable format such as JSON. This allows automated testing of the data generator and the individual data flows to verify correctness.
\item Data flows that contain configurable properties include precise descriptions of the effects of the properties, including their valid domains. A lack of clarity on what the effect of a property is makes it hard to estimate what the change of the property is going to evaluate about the system.
\item The set of data flows includes a variety of operators in short graphs. This allows the evaluation of individual operators to estimate their costs as basic entities. Including map, filter, reduce, window, and join operators is absolutely vital.
\item The set of data flows includes complex graphs that combine a variety of operators. This allows the evaluation of the overall system performance when dealing with deeper graphs and longer computations and shows the system's performance for the application to practical problems.
\item The benchmark includes test setups that evaluate not only input to output latency, but also congestion behaviour, scalability, and fault tolerance. Real systems experience load spikes and failures on machines. It is thus essential for a benchmark to include these properties in its evaluation model.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
