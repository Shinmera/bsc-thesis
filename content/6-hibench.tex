\section{HiBench: A Cross-Platforms Micro-Benchmark Suite for Big Data}
HiBench\cite{hibench} is a benchmarking suite created by Intel in 2012. It proposes a set of microbenchmarks to test Big Data processing systems. It includes implementations of the tests for Spark, Flink, Storm, and Gearpump. For our purposes in testing Strymon, we will focus only on the four tests of the streaming suite:

\begin{itemize}
\item \tb{Identity} This test is supposed to measure the minimum latency of the system, by simply immediately outputting the input data.
  \imagefigure[hibench-identity]{images/hibench-1.pdf}{HiBench's Identity dataflow graph}
\item \tb{Repartition} This tests the distribution of the workload across workers, but just like Identity does not perform any computation on the data. The repartition should be handled through a round-robin scheduler.
  \imagefigure[hibench-repartition]{images/hibench-2.pdf}{HiBench's Repartition dataflow graph}
\item \tb{Wordcount} This is a basic word count test that simply focuses on counting the occurrences of individual words, regularly outputting the current tally. It is intended to test the performance of stateful operators.
  \imagefigure[hibench-wordcount]{images/hibench-3.pdf}{HiBench's Wordcount dataflow graph}
\item \tb{Fixwindow} This test performs a simple hopping window reduction, with each window being ten seconds long.
  \imagefigure[hibench-fixwindow]{images/hibench-4.pdf}{HiBench's Fixwindow dataflow graph}
\end{itemize}

The data used for the benchmark follows a custom CSV-like format, where each input is composed of an integer timestamp and a comma separated list of the following fields:

\begin{itemize}
\item \code{ip} An IPv4 address, presumably for the event origin.
\item \code{session_id} A unique session ID hash.
\item \code{date} Some kind of date in \code{YYYY-MM-DD} format.
\item \code{?} A float of some kind.
\item \code{user_agent} A browser user-agent string, to identify the user.
\item \code{?} Some three-letter code.
\item \code{?} Some five-letter sub-code.
\item \code{word} A seemingly random word.
\item \code{?} Some integer.
\end{itemize}

As the benchmark does not publicly state the structure of the workload, and the fields aren't really specifically used for anything in the benchmarks except for the \code{word}, we can only guess what they are meant to be for. \\

Since the benchmarks focus on very small tests, they can only really give insight about the performance of the system for a select few individual operations. This might not translate to the performance of the system for complex data flows with many interacting components. Hibench only focuses on the latency component of the system, measuring how long it takes the system to process data at a fixed input rate. It does not consider other important factors of a streaming system such as fault tolerance, scaling, and load bearing.

\subsection{Implementation}
\subsubsection{Data Generation}
As the data generation process is not documented explicitly anywhere and the source is rather hard to decode, we opted for a much simpler scheme that should nevertheless follow the overall structure of the data used in HiBench. Our generator produces a fixed-size set of random IPs, by default set to 100. It then generates events at a fixed number of events per second, with the assumption that the timestamps in the data records correspond to seconds. For each event, the \code{ip} is chosen at random from the set, the \code{session_id} is a random 54 character long ASCII string, and the \code{date} is a randomly generated date string in the appropriate format. All of the remaining fields are left the same across all events. \\

Since, as far as we can tell, only the IP and timestamp are actually used by any of the streaming queries, we do not believe that the lack of proper data generation for the remaining fields severely skews our workloads. This observation is based on the Flink implementation of HiBench.

\subsubsection{Data Flows}\label{section:hibench-queries}
\paragraph{Identity}
\begin{listing}[H]
  \inputminted[firstline=95,lastline=96]{rust}{benchmarks/src/hibench.rs}
  \caption{Implementation for the Identity query.}
  \label{lst:hibench-identity}
\end{listing}

In this query we simply parse out the timestamp from its string representation and return it alongside the current number of seconds since the UNIX epoch.

\paragraph{Repartition}
\begin{listing}[H]
  \inputminted[firstline=127,lastline=139]{rust}{benchmarks/src/hibench.rs}
  \caption{Implementation for the Repartition query.}
  \label{lst:hibench-repartition}
\end{listing}

This is the most complex implementation of all queries in the HiBench set, since HiBench expects the data exchange to be performed in a round-robin fashion, whereas Timely usually performs a hashing scheme to exchange data between nodes on different workers. There is currently no built-in operator to perform round-robin exchanges, so we have to simulate it with an ad-hoc implementation here. We do this by mapping each input to a tuple of current round-robin count and record. We then use this round-robin count in order to use the usual \code{exchange} operator. A more efficient implementation would handle the exchange between workers directly.

\paragraph{Wordcount}
\begin{listing}[H]
  \inputminted[firstline=170,lastline=172]{rust}{benchmarks/src/hibench.rs}
  \caption{Implementation for the WordCount query.}
  \label{lst:hibench-wordcount}
\end{listing}

For a word count, all we really need is the \code{rolling_count} operator, which performs a continuously updating reduction. In order to achieve a more efficient counting scheme, we exchange each record between workers hashed on the IP. This means that each worker will receive a disjoint set of IPs to count, making the reduction much more efficient.

\paragraph{Fixwindow}
\begin{listing}[H]
  \inputminted[firstline=205,lastline=209]{rust}{benchmarks/src/hibench.rs}
  \caption{Implementation for the Fixwindow query.}
  \label{lst:hibench-fixwindow}
\end{listing}

For this query, we merely need to create a tumbling window for ten seconds, and then count the number of events per IP in the window as well as their minimal timestamp, both of which can be achieved with a single \code{reduce_by}.

\subsection{Evaluation}
We ran our experiments on an AMD Opteron 6378 2.4KHz 64bit machine with a total of 32 Cores and 504GB RAM. This machine is known to exhibit strange scaling behaviour due to non-uniform memory access patterns. You can see this behaviour in the \hyperref[figure:ysb-scaling]{scaling plot} at 16 workers. \\

Our measurement procedure involved a closed-loop data feed, meaning each epoch was run to completion before a new epoch with a new round of data was started. We only measured data flow execution time, excluding data generation time. Each measurement was gathered using 300 epochs of data. \\

\latencyfigure[nex]{32}{1,2,3,4}{Identity, Repartition, Wordcount, Fixwindow}
\scalingfigure[nex]{10000000}{1,2,3,4}{Identity, Repartition, Wordcount, Fixwindow}
\cdffigure[nex]{32}{10000000}{1,2,3,4}{Identity, Repartition, Wordcount, Fixwindow}

We surmise that the exceedingly poor performance of the Identity query is down to the requirement of including a current Unix time measurement, which requires a system call that bottlenecks the data flow. The repartition dataflow scales poorly as Timely automatically performs a hash-based redistribution of data for operators that require exchange and we automatically feed data on all workers by default, distributing the data from the get-go.

\subsection{Remarks}\label{section:hibench-remarks}
Surprisingly enough, HiBench gave us a lot of trouble to implement. Not because the queries were complex, but simply because of the lack of proper documentation and maintenance. Beyond the very superficial descriptions of the queries on their homepage, there is nothing about how the workloads are generated, how the queries perform in detail, or what the though process behind the design was. The code base itself is not easy to decipher either, as the information about data generation is distributed over a swath of files, none of which are commented or explained anywhere. \\

If this didn't already make things bad enough, the benchmark itself does not run on current setups. It requires versions of Hadoop and Kafka that are no longer supported, and does not work under Java 9. We could not get their data generator to work on several systems. We are unsure whether this was due to a misconfiguration somewhere or due to the software being bitrotten, but the \href{https://github.com/intel-hadoop/HiBench/issues/535}{authors did not respond to inquiries}, and their documentation for the setup of the benchmark is feeble at best. \\

While we do see some worth in having a very minimal benchmark that focuses on testing the performance of individual operators, we are not convinced that such information could be used to infer meaningful data about a streaming system as a whole, and especially not about its expressiveness and capability to handle larger dataflows. \\

If a benchmark such as this were formulated again, it is absolutely vital that the authors properly document the data structures used and how they're generated, as well as the exact computation a query should perform. Without this, it is hardly feasible for third-parties to implement the benchmark for their own system and arrive at comparable timing data.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
