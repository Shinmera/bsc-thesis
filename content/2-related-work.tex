\section{Related Work}
In this section we analyse and compare a number of papers about stream processors. In particular, we look at the ways in which they evaluate and test their systems in order to get an idea of how benchmarking has so far commonly been done. Each subsection looks at one paper at a time, providing a graph of the data flows and operators used to evaluate the system, if such information was available.

\subsection{\citefulltitle{s4}}
The S4 paper evaluates its performance with two sample algorithms: click-through rate (CTR), and online parameter optimisation (OPO). The system uses typed events between nodes and distributes them to physical nodes based on a ``key property'' on the event. \\

The CTR test is implemented by four nodes: the first assigns keys to the initially keyless events coming in. It passes them to a node that combines matching events. From there the events go on to a filter that removes unwanted events. Finally, the events are passed to a node that computes the CTR, and emits it as a new event. \\

The OPO test consists of five nodes: the first node assigns keys to route the events to either the second or third node. The second and third nodes perform some computations on the events and emit the results as new ones. The fourth node compares the events it gets from nodes two and three, to determine the optimisation parameters. The fifth node runs an adaptation depending on the parameters it receives.

\imagefigure{images/s4-graphs.pdf}{Graphs of the two tests used to evaluate the S4 platform: click-through rate and online parameter optimisation. Orange nodes are stateful. Nodes coloured in red retain state of previous stream events.}

\subsection{\citefulltitle{spade}}
\imagefigure{images/spade-graphs.pdf}{Graph of the example used in the SPADE paper: a bargain index computation. Nodes coloured in red retain state of previous stream events.}

\subsection{\citefulltitle{discretized}}
\imagefigure{images/discretized-graphs.pdf}{Graph of the Word Count example used to illustrate the discretized streams. Not shown is the incremental reduction done between batches.}

\subsection{\citefulltitle{millwheel}}

\subsection{\citefulltitle{bigdatabench}}

\subsection{\citefulltitle{streamcloud}}
\imagefigure{images/streamcloud-graphs.pdf}{Graph of the query used to evaluate StreamCloud. Nodes coloured in red retain state of previous stream events.}

\subsection{\citefulltitle{integrating}}
\imagefigure{images/integrating-graphs.pdf}{Illustration of the queries for the Linear Road Benchmark (top) and the Top-K (bottom) tests used to evaluate their system. Orange nodes maintain internal state.}

\subsection{\citefulltitle{timestream}}
\imagefigure{images/timestream-graphs.pdf}{The Distinct Count query used to evaluate the Timestream system. Nodes in orange have internal state.}

The sentiment analysis has no description of the graph used and we could thus not replicate it here.

\subsection{\citefulltitle{storm}}
\imagefigure{images/storm-graphs.pdf}{The topology graph used to evaluate the Storm schedulers. Orange nodes are stateful. ``Shuffle'' connections send the output event to a random destination node. ``Fields'' connections send the output event to a specific destination node based on a field of the event.}

\subsection{\citefulltitle{storm2}}
\imagefigure{images/storm2-graphs.pdf}{An illustration of the topology used for the Twitter \& Bitly link trend analysis. Nodes coloured orange maintain internal state.}

\subsection{Comparison}
In \autoref{table:test-properties} and \autoref{table:test-setups} we compare the most important features of the tests performed in the various papers. Unfortunately, most of the papers do not supply or use publicly available data, making it difficult to compare them, even if the test data flow were replicated.

\begin{table}[H]
  \centering
  {
    \scriptsize
    \hspace*{-1cm}
    \begin{tabular}{|p{3cm}|p{4cm}|p{3cm}|p{2.5cm}|p{2.5cm}|}
      \hline
      Paper & Goal & Application & Dataflow Properties & Dataflow Operators
      \\\hline
      
      S4\cite{s4}
      & A practical application of the system to a real-life problem.
      & Search
      & Stateful
      & Map, Filter, Join
      \\\hline
      
      SPADE\cite{spade}
      & Sample application, performance study.
      & Finance
      & Stateful
      & Map, Filter, Aggregate, Join
      \\\hline

      D-Streams\cite{discretized}
      & Scalability and recovery test.
      & None
      & None
      & Map, Window Aggregate
      \\\hline

      Millwheel\cite{millwheel}
      & In-Out Latency.
      & Ads
      & Unspecified
      & Unspecified
      \\\hline

      BigDataBench\cite{bigdatabench}
      & Fair performance evaluation of big data systems.
      & Search, Social, Commerce
      & Unspecified
      & Unspecified
      \\\hline

      StreamCloud\cite{streamcloud}
      & Evaluation of scalability and elasticity.
      & Telephony
      & Stateful
      & Map, Filter, Aggregate, Join
      \\\hline

      Operator State\cite{integrating}
      & Testing dynamic scaling and fault-tolerance.
      & Road tolls
      & Stateful
      & Map, Aggregate, Join
      \\\hline

      TimeStream\cite{timestream}
      & Low-latency test for real-world applications.
      & Search, Social Network
      & None
      & Map, Filter, Aggregate, Window
      \\\hline

      Adaptive Scheduling\cite{storm}
      & Evaluating performance of scheduling algorithms.
      & None
      & Stateful
      & None
      \\\hline

      Analytics on High Velocity Streams\cite{storm2}
      & Analysing trends for links on Twitter.
      & Social Network
      & Stateful
      & Map, Filter, Aggregate, Window
      \\\hline

      YSB\cite{ysb}
      & Benchmarking streaming systems via Ad analytics.
      & Ads
      & Stateful
      & Map, Filter, Aggregate, Join, Window
      \\\hline

      HiBench\cite{hibench}
      & Evaluating big data processing systems.
      & Big Data
      & Stateful
      & Map, Aggregate, Window
      \\\hline

      NEXMark\cite{nexmark}
      & Adaptation of XMark for streaming systems.
      & Auctioning
      & {\bfseries TODO}
      & {\bfseries TODO}
      \\\hline
    \end{tabular}
  }
  \caption{Comparison of the test properties of the reference papers.}
  \label{table:test-properties}
\end{table}

\begin{table}[H]
  \centering
  {
    \scriptsize
    \hspace*{-1cm}
    \begin{tabular}{|p{3cm}|p{3.5cm}|p{4cm}|p{2.5cm}|p{2cm}|}
      \hline
      Paper & Workloads & Testbed & External Systems & Public Data
      \\\hline
      
      S4\cite{s4}
      & \textasciitilde 1M live events per day for two weeks.
      & 16 servers with 4x32-bit CPUs, 2GB RAM each.
      & Unspecified
      & No
      \\\hline
      
      SPADE\cite{spade}
      & \textasciitilde 250M transactions, resulting in about 20GB of data.
      & 16 cluster nodes. Further details not available.
      & IBM GPFS
      & Maybe\tablefootnote{The data was retrieved from the IBM WebSphere Web Front Office for all of December 2005.}
      \\\hline

      D-Streams\cite{discretized}
      & \textasciitilde 20 MB/s/node (200K records/s/node) for WordCount.
      & Up to 60 Amazon EC2 nodes, 4 cores, 15GB RAM each.
      & Unspecified
      & No
      \\\hline

      Millwheel\cite{millwheel}
      & Unspecified.
      & 200 CPUs. Nothing further is specified.
      & BigTable
      & No
      \\\hline

      BigDataBench\cite{bigdatabench}
      & Up to 1TB.
      & 15 nodes, Xeon E5645, 16GB RAM, 8TB disks each.
      & Hadoop, MPI, Hbase, Hive, MySQL
      & Yes\tablefootnote{Obtainable at \url{http://prof.ict.ac.cn/BigDataBench/}}
      \\\hline

      StreamCloud\cite{streamcloud}
      & Up to 450'000 transactions per second.
      & 100 nodes, 32 cores, 8GB RAM, 0.5TB disks each, 1Gbit LAN.
      & Unspecified
      & No
      \\\hline

      Operator State\cite{integrating}
      & Up to 600'000 tuples/s.
      & Up to 50 Amazon EC2 ``small'' instances with 1.7GB RAM.
      & Unspecified
      & No
      \\\hline

      TimeStream\cite{timestream}
      & \textasciitilde 30M URLs, \textasciitilde 1.2B Tweets.
      & Up to 16 Dual Xeon X3360 2.83GHz, 8GB RAM, 2TB disks each, 1Gbit LAN.
      & Unspecified
      & No
      \\\hline

      Adaptive Scheduling\cite{storm}
      & Generated.
      & 8 nodes, 2x2.8GHz CPU, 3GB RAM, 15GB disks each, 10Gbit LAN.
      & Nimbus, Zookeper
      & Yes\tablefootnote{The data is generated on the fly, the algorithm of which is specified in the paper.}
      \\\hline

      Analytics on High Velocity Streams\cite{storm2}
      & \textasciitilde 1'600GB of compressed text data.
      & 4 nodes, Intel i7-2600 CPU, 8GB RAM each.
      & Kafka, Cassandra
      & Maybe\tablefootnote{Data stems from Twitter and Bit.ly for June of 2012, but is not publicly available.}
      \\\hline

      YSB\cite{ysb}
      & Generated
      & Unspecified
      & Kafka, Redis
      & Yes\tablefootnote{Generated by YSB: \url{https://github.com/yahoo/streaming-benchmarks}}
      \\\hline

      HiBench\cite{hibench}
      & Generated
      & Unspecified
      & Kafka
      & Yes\tablefootnote{Generated by HiBench.}
      \\\hline

      NEXMark\cite{nexmark}
      & Generated
      & Unspecified
      & Firehose Stream Generator
      & Yes\tablefootnote{Generated by the ``Firehose Stream Generator''.}
      \\\hline

    \end{tabular}
  }
  \caption{Comparison of the test setups of the reference papers.}
  \label{table:test-setups}
\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% TeX-engine: luatex
%%% End:
